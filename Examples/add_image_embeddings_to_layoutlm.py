# -*- coding: utf-8 -*-
"""Add image embeddings to LayoutLM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Add_image_embeddings_to_LayoutLM.ipynb

In this notebook, we are going to fine-tune LayoutLM on the [FUNSD dataset](https://guillaumejaume.github.io/FUNSD/), and we will add **visual features from a pre-trained backbone** (namely ResNet-101) as was done in the original paper. The parameters of this backbone will also be updated during training.

As we'll see, this will improve performance compared to a model only trained on text + layout (bounding boxes) information (which is also available in my repo "Transformers-Tutorials").

* Original LayoutLM paper: https://arxiv.org/abs/1912.13318

* Original FUNSD paper: https://arxiv.org/abs/1905.13538
"""

!pip install -q transformers seqeval

"""Let's start by reading in the data of the FUNSD dataset."""

! wget https://guillaumejaume.github.io/FUNSD/dataset.zip
! unzip dataset.zip && mv dataset data && rm -rf dataset.zip __MACOSX

"""## Demo: adding visual embeddings to LayoutLM

Here we illustrate how we can provide additional visual information to LayoutLM for a single document image.

Let's take a look at a random document:
"""

from PIL import Image, ImageDraw, ImageFont

image = Image.open("/content/data/training_data/images/0000971160.png")
image = image.convert("RGB")
image

"""As we can see, the resolution is arbitrary (not square):"""

image.size

"""Let's also consider the corresponding annotations (words, bounding boxes and labels):"""

import json

with open('/content/data/training_data/annotations/0000971160.json') as f:
  data = json.load(f)

words = []
bounding_boxes = []
labels = []
for annotation in data['form']:
  # get label
  label = annotation['label']
  # get words
  for annotated_word in annotation['words']:
      if annotated_word['text'] == '':
        continue
      words.append(annotated_word['text'])
      bounding_boxes.append(annotated_word['box'])
      labels.append(label)

print("Words:", words)
print("Bounding boxes:", bounding_boxes)
print("Labels:", labels)

"""We have 145 words with corresponding bounding boxes and sequence labels for the given document."""

assert len(words) == len(bounding_boxes) == len(labels)
print(len(words))

"""Let's visualize the bounding boxes (we use a single color for all labels for now):"""

image_with_bboxes = image.copy()

draw = ImageDraw.Draw(image_with_bboxes, "RGBA")
for bbox in bounding_boxes:
    draw.rectangle(bbox, outline='red', width=1)

image_with_bboxes

"""Future to do: sort the words/bounding boxes/labels from top left in the document to bottom right. We can do this based on the coordinates of the bounding boxes. Checking the [FUNSD paper](https://arxiv.org/abs/1905.13538), the bounding boxes are in = `[xleft, ytop, xright, ybottom]` format.

#### Resize image + bounding boxes

Next, we resize the image to 224x224 in order to provide it to a pre-trained backbone CNN (we'll use ResNet-101 pre-trained on ImageNet, available in `torchvision`). We have to resize the bounding boxes accordingly.

Source: https://stackoverflow.com/questions/61315541/resnet-101-featuremap-shape
"""

from PIL import Image
import numpy as np

# resize image
target_size = 224
resized_image = image.copy().resize((target_size, target_size))

# resize corresponding bounding boxes (annotations)
# Thanks, Stackoverflow: https://stackoverflow.com/questions/49466033/resizing-image-and-its-bounding-box
def resize_bounding_box(bbox, original_image, target_size):
  x_, y_ = original_image.size

  x_scale = target_size / x_ 
  y_scale = target_size / y_
  
  origLeft, origTop, origRight, origBottom = tuple(bbox)
  
  x = int(np.round(origLeft * x_scale))
  y = int(np.round(origTop * y_scale))
  xmax = int(np.round(origRight * x_scale))
  ymax = int(np.round(origBottom * y_scale))
  
  return [x, y, xmax, ymax]

resized_bounding_boxes = [resize_bounding_box(bbox, image, target_size) for bbox in bounding_boxes]

draw = ImageDraw.Draw(resized_image, "RGBA")
for bbox in resized_bounding_boxes:
    draw.rectangle(bbox, outline='red', width=1)

resized_image

"""#### Provide resized image to backbone 

Next, we provide the resized image (224x224) to the pre-trained CNN backbone to obtain a feature map.
"""

from torchvision.transforms import ToTensor

image = ToTensor()(resized_image).unsqueeze(0) # batch size of 1
image.shape

"""This feature map has a much higher channel dimension (1024 compared to 3 in the original image), but has a smaller spatial scale (14x14 compared to 224x224 in the original image)."""

import torch
import torchvision
from torchvision.transforms import ToTensor

# load resnet101 model and remove the last layer
model = torchvision.models.resnet101(pretrained=True)
model = torch.nn.Sequential(*(list(model.children())[:-3]))

with torch.no_grad():
    feature_map = model(image)

print(feature_map.size())  # torch.Size([1, 1024, 14, 14])

"""#### Use ROI-align for bounding boxes

Next, we use ROI-align to get feature maps for each individual resized bounding box.

In order to understand ROI-align, I highly recommend [this video](https://www.youtube.com/watch?v=9AyMR4IhSWQ&list=PL5-TkQAfAZFbzxjBHtzdVCWE0Zbhomg7r&index=16&ab_channel=MichiganOnline) which helped me a lot!

Also, [this Stackoverflow post](https://stackoverflow.com/questions/60060016/why-does-roi-align-not-seem-to-work-in-pytorch) helped me a lot in understanding how this is implemented in PyTorch.
"""

import torch
from torchvision.ops import RoIAlign

output_size = (3,3)
spatial_scale = feature_map.shape[2]/target_size # 14/224
sampling_ratio = 2  

roi_align = RoIAlign(output_size, spatial_scale=spatial_scale, sampling_ratio=sampling_ratio)

def align_bounding_boxes(bounding_boxes):
    aligned_bounding_boxes = []
    for bbox in bounding_boxes:
        aligned_bbox = [bbox[0] - 0.5, bbox[1] - 0.5, bbox[2] + 0.5, bbox[3] + 0.5]
        aligned_bounding_boxes.append(aligned_bbox)

    return aligned_bounding_boxes

feature_maps_bboxes = roi_align(input=feature_map, 
                                # we pass in a single tensor, with each bounding box also containing the batch index (0)
                                # We also add -0.5 for the first two coordinates and +0.5 for the last two coordinates,
                                # see https://stackoverflow.com/questions/60060016/why-does-roi-align-not-seem-to-work-in-pytorch
                                rois=torch.tensor([[0] + bbox for bbox in align_bounding_boxes(resized_bounding_boxes)]).float()
                      )
print(feature_maps_bboxes.shape)

"""#### Turn feature maps into visual embeddings

To turn the feature maps for each bounding box into a visual embedding, we first flatten them:
"""

visual_embeddings = torch.flatten(feature_maps_bboxes, 1)
visual_embeddings.shape

"""Next, we use a linear projection layer to match the dimensions of LayoutLM:"""

import torch.nn as nn

projection = nn.Linear(in_features=visual_embeddings.shape[-1], out_features=768)
output = projection(visual_embeddings)
print(output.shape)

"""## Define PyTorch dataset and dataloaders

Now that we have shown how this works for a single document, let's define a PyTorch dataset and dataloaders.

Let's first create 2 lists: image files names for training and testing.

"""

from os import listdir

# list all training image file names
image_files_train = [f for f in listdir('/content/data/training_data/images')]
# list all test image file names
image_files_test = [f for f in listdir('/content/data/testing_data/images')]

"""Let's also create a dictionary that maps labels to indices and vice versa."""

labels = ['B-answer', 'I-answer', 'B-header', 'I-header', 'B-question', 'I-question', 'B-other', 'I-other']
labels

idx2label = {v: k for v, k in enumerate(labels)}
label2idx = {k: v for v, k in enumerate(labels)}
label2idx

"""Now let's define the PyTorch dataset:"""

from torch.utils.data import Dataset, DataLoader
from PIL import Image
import json
import numpy as np
from torchvision.transforms import ToTensor
import torch

def normalize_box(box, width, height):
     return [
         int(1000 * (box[0] / width)),
         int(1000 * (box[1] / height)),
         int(1000 * (box[2] / width)),
         int(1000 * (box[3] / height)),
     ]

# resize corresponding bounding boxes (annotations)
# Thanks, Stackoverflow: https://stackoverflow.com/questions/49466033/resizing-image-and-its-bounding-box
def resize_and_align_bounding_box(bbox, original_image, target_size):
  x_, y_ = original_image.size

  x_scale = target_size / x_ 
  y_scale = target_size / y_
  
  origLeft, origTop, origRight, origBottom = tuple(bbox)
  
  x = int(np.round(origLeft * x_scale))
  y = int(np.round(origTop * y_scale))
  xmax = int(np.round(origRight * x_scale))
  ymax = int(np.round(origBottom * y_scale)) 
  
  return [x-0.5, y-0.5, xmax+0.5, ymax+0.5]

class FUNSDDataset(Dataset):
    """LayoutLM dataset with visual features."""

    def __init__(self, image_file_names, tokenizer, max_length, target_size, train=True):
        self.image_file_names = image_file_names
        self.tokenizer = tokenizer
        self.max_seq_length = max_length
        self.target_size = target_size
        self.pad_token_box = [0, 0, 0, 0]
        self.train = train

    def __len__(self):
        return len(self.image_file_names)

    def __getitem__(self, idx):

        # first, take an image
        item = self.image_file_names[idx]
        if self.train:
          base_path = "/content/data/training_data"
        else:
          base_path = "/content/data/testing_data"
        
        original_image = Image.open(base_path + "/images/" + item).convert("RGB")
        # resize to target size (to be provided to the pre-trained backbone)
        resized_image = original_image.resize((self.target_size, self.target_size))
        
        # first, read in annotations at word-level (words, bounding boxes, labels)
        with open(base_path + '/annotations/' + item[:-4] + '.json') as f:
          data = json.load(f)
        words = []
        unnormalized_word_boxes = []
        word_labels = []
        for annotation in data['form']:
          # get label
          label = annotation['label']
          # get words
          for annotated_word in annotation['words']:
              if annotated_word['text'] == '':
                continue
              words.append(annotated_word['text'])
              unnormalized_word_boxes.append(annotated_word['box'])
              word_labels.append(label)

        width, height = original_image.size
        normalized_word_boxes = [normalize_box(bbox, width, height) for bbox in unnormalized_word_boxes]
        assert len(words) == len(normalized_word_boxes)

        # next, transform to token-level (input_ids, attention_mask, token_type_ids, bbox, labels)
        token_boxes = []
        unnormalized_token_boxes = []
        token_labels = []
        for word, unnormalized_box, box, label in zip(words, unnormalized_word_boxes, normalized_word_boxes, word_labels):
            word_tokens = self.tokenizer.tokenize(word)
            unnormalized_token_boxes.extend(unnormalized_box for _ in range(len(word_tokens)))
            token_boxes.extend(box for _ in range(len(word_tokens)))
            # label first token as B-label (beginning), label all remaining tokens as I-label (inside)
            for i in range(len(word_tokens)):
              if i == 0:
                token_labels.extend(['B-' + label])
              else:
                token_labels.extend(['I-' + label])
        
        # Truncation of token_boxes + token_labels
        special_tokens_count = 2 
        if len(token_boxes) > self.max_seq_length - special_tokens_count:
            token_boxes = token_boxes[: (self.max_seq_length - special_tokens_count)]
            unnormalized_token_boxes = unnormalized_token_boxes[: (self.max_seq_length - special_tokens_count)]
            token_labels = token_labels[: (self.max_seq_length - special_tokens_count)]
        
        # add bounding boxes and labels of cls + sep tokens
        token_boxes = [[0, 0, 0, 0]] + token_boxes + [[1000, 1000, 1000, 1000]]
        unnormalized_token_boxes = [[0, 0, 0, 0]] + unnormalized_token_boxes + [[1000, 1000, 1000, 1000]]
        token_labels = [-100] + token_labels + [-100]
        
        encoding = self.tokenizer(' '.join(words), padding='max_length', truncation=True)
        # Padding of token_boxes up the bounding boxes to the sequence length.
        input_ids = self.tokenizer(' '.join(words), truncation=True)["input_ids"]
        padding_length = self.max_seq_length - len(input_ids)
        token_boxes += [self.pad_token_box] * padding_length
        unnormalized_token_boxes += [self.pad_token_box] * padding_length
        token_labels += [-100] * padding_length
        encoding['bbox'] = token_boxes
        encoding['labels'] = token_labels

        assert len(encoding['input_ids']) == self.max_seq_length
        assert len(encoding['attention_mask']) == self.max_seq_length
        assert len(encoding['token_type_ids']) == self.max_seq_length
        assert len(encoding['bbox']) == self.max_seq_length
        assert len(encoding['labels']) == self.max_seq_length

        encoding['resized_image'] = ToTensor()(resized_image)
        # rescale and align the bounding boxes to match the resized image size (typically 224x224) 
        encoding['resized_and_aligned_bounding_boxes'] = [resize_and_align_bounding_box(bbox, original_image, self.target_size) 
                                                          for bbox in unnormalized_token_boxes]

        encoding['unnormalized_token_boxes'] = unnormalized_token_boxes
        
        # finally, convert everything to PyTorch tensors 
        for k,v in encoding.items():
            if k == 'labels':
              label_indices = []
              # convert labels from string to indices
              for label in encoding[k]:
                if label != -100:
                  label_indices.append(label2idx[label])
                else:
                  label_indices.append(label)
              encoding[k] = label_indices
            encoding[k] = torch.as_tensor(encoding[k])
        
        return encoding

from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
train_dataset = FUNSDDataset(image_file_names=image_files_train, tokenizer=tokenizer, max_length=512, target_size=224)

"""Let's verify an example:"""

encoding = train_dataset[0]
encoding.keys()

tokenizer.decode(encoding.input_ids)

encoding['resized_image'].shape

from torchvision.transforms import ToPILImage

test_image = ToPILImage()(encoding['resized_image']).convert("RGB")
test_image

draw = ImageDraw.Draw(test_image, "RGBA")
for bbox in encoding['resized_and_aligned_bounding_boxes'].tolist():
    draw.rectangle(bbox, outline='red', width=1)

test_image

"""We can now define a training dataloader:"""

train_dataloader = DataLoader(train_dataset, batch_size=4)
batch = next(iter(train_dataloader))

for k,v in batch.items():
  print(k, batch[k].shape)

# this is how we can provide resized and aligned bounding boxes to the roi align (as a list of Tensors)
for i in batch['resized_and_aligned_bounding_boxes']:
  print(i.shape)

"""## Define model

So if we want to define a model that includes LayoutLM + the visual embeddings, it would look like this:
"""

import torch.nn as nn
from transformers.models.layoutlm import LayoutLMModel, LayoutLMConfig
from transformers.modeling_outputs import TokenClassifierOutput
import torchvision
from torchvision.ops import RoIAlign

class LayoutLMForTokenClassification(nn.Module):
    def __init__(self, output_size=(3,3), 
                 spatial_scale=14/224, 
                 sampling_ratio=2
        ): 
        super().__init__()
        
        # LayoutLM base model + token classifier
        self.num_labels = len(label2idx)
        self.layoutlm = LayoutLMModel.from_pretrained("microsoft/layoutlm-base-uncased", num_labels=self.num_labels)
        self.dropout = nn.Dropout(self.layoutlm.config.hidden_dropout_prob)
        self.classifier = nn.Linear(self.layoutlm.config.hidden_size, self.num_labels)

        # backbone + roi-align + projection layer
        model = torchvision.models.resnet101(pretrained=True)
        self.backbone = nn.Sequential(*(list(model.children())[:-3]))
        self.roi_align = RoIAlign(output_size, spatial_scale=spatial_scale, sampling_ratio=sampling_ratio)
        self.projection = nn.Linear(in_features=1024*3*3, out_features=self.layoutlm.config.hidden_size)

    def forward(
        self,
        input_ids,
        bbox,
        attention_mask,
        token_type_ids,
        position_ids=None,
        head_mask=None,
        inputs_embeds=None,
        labels=None,
        resized_images=None, # shape (N, C, H, W), with H = W = 224
        resized_and_aligned_bounding_boxes=None, # single torch tensor that also contains the batch index for every bbox at image size 224
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
    ):
        r"""
        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -
            1]``.

        """
        return_dict = return_dict if return_dict is not None else self.layoutlm.config.use_return_dict

        # first, forward pass on LayoutLM
        outputs = self.layoutlm(
            input_ids=input_ids,
            bbox=bbox,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
            position_ids=position_ids,
            head_mask=head_mask,
            inputs_embeds=inputs_embeds,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )

        sequence_output = outputs[0]

        # next, send resized images of shape (batch_size, 3, 224, 224) through backbone to get feature maps of images 
        # shape (batch_size, 1024, 14, 14)
        feature_maps = self.backbone(resized_images)
        
        # next, use roi align to get feature maps of individual (resized and aligned) bounding boxes
        # shape (batch_size*seq_len, 1024, 3, 3)
        device = input_ids.device
        resized_bounding_boxes_list = []
        for i in resized_and_aligned_bounding_boxes:
          resized_bounding_boxes_list.append(i.float().to(device))
                
        feat_maps_bboxes = self.roi_align(input=feature_maps, 
                                        # we pass in a list of tensors
                                        # We have also added -0.5 for the first two coordinates and +0.5 for the last two coordinates,
                                        # see https://stackoverflow.com/questions/60060016/why-does-roi-align-not-seem-to-work-in-pytorch
                                        rois=resized_bounding_boxes_list
                           )  
      
        # next, reshape  + project to same dimension as LayoutLM. 
        batch_size = input_ids.shape[0]
        seq_len = input_ids.shape[1]
        feat_maps_bboxes = feat_maps_bboxes.view(batch_size, seq_len, -1) # Shape (batch_size, seq_len, 1024*3*3)
        projected_feat_maps_bboxes = self.projection(feat_maps_bboxes) # Shape (batch_size, seq_len, hidden_size)

        # add those to the sequence_output - shape (batch_size, seq_len, hidden_size)
        sequence_output += projected_feat_maps_bboxes

        sequence_output = self.dropout(sequence_output)
        logits = self.classifier(sequence_output)

        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()

            if attention_mask is not None:
                active_loss = attention_mask.view(-1) == 1
                active_logits = logits.view(-1, self.num_labels)[active_loss]
                active_labels = labels.view(-1)[active_loss]
                loss = loss_fct(active_logits, active_labels)
            else:
                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))

        if not return_dict:
            output = (logits,) + outputs[2:]
            return ((loss,) + output) if loss is not None else output

        return TokenClassifierOutput(
            loss=loss,
            logits=logits,
            hidden_states=outputs.hidden_states,
            attentions=outputs.attentions,
        )

model = LayoutLMForTokenClassification()

"""Let's verify a forward pass on a batch:"""

batch.keys()

input_ids=batch['input_ids']
bbox=batch['bbox']
attention_mask=batch['attention_mask']
token_type_ids=batch['token_type_ids']
labels=batch['labels']
resized_images = batch['resized_image'] # shape (N, C, H, W), with H = W = 224
resized_and_aligned_bounding_boxes = batch['resized_and_aligned_bounding_boxes'] # single torch tensor that also contains the batch index for every bbox at image size 224

outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids, 
                labels=labels, resized_images=resized_images, resized_and_aligned_bounding_boxes=resized_and_aligned_bounding_boxes)

outputs.loss

outputs.logits.shape

"""## Train the model

Next, we can train the model in regular PyTorch fashion.
"""

from transformers import AdamW
from tqdm.notebook import tqdm

optimizer = AdamW(model.parameters(), lr=5e-5)

global_step = 0
num_train_epochs = 10

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

#put the model in training mode
model.to(device)
model.train()
for epoch in range(num_train_epochs):
  print("Epoch:", epoch)
  for batch in tqdm(train_dataloader):
      # forward pass
      input_ids=batch['input_ids'].to(device)
      bbox=batch['bbox'].to(device)
      attention_mask=batch['attention_mask'].to(device)
      token_type_ids=batch['token_type_ids'].to(device)
      labels=batch['labels'].to(device)
      resized_images = batch['resized_image'].to(device) 
      resized_and_aligned_bounding_boxes = batch['resized_and_aligned_bounding_boxes'].to(device) 

      outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids, 
                      labels=labels, resized_images=resized_images, resized_and_aligned_bounding_boxes=resized_and_aligned_bounding_boxes)
      loss = outputs.loss

      # print loss every 10 steps
      if global_step % 10 == 0:
        print(f"Loss after {global_step} steps: {loss.item()}")

      # backward pass to get the gradients 
      loss.backward()

      # update
      optimizer.step()
      optimizer.zero_grad()
      global_step += 1

"""## Test on test set

Finally, we can test our trained model on the FUNSD test set.
"""

test_dataset = FUNSDDataset(image_file_names=image_files_test, tokenizer=tokenizer, max_length=512, target_size=224, train=False)
test_dataloader = DataLoader(test_dataset, batch_size=4)

import numpy as np
from seqeval.metrics import (
    classification_report,
    f1_score,
    precision_score,
    recall_score,
)

eval_loss = 0.0
nb_eval_steps = 0
preds = None
out_label_ids = None

# put model in evaluation mode
model.eval()
for batch in tqdm(test_dataloader, desc="Evaluating"):
    with torch.no_grad():
        input_ids=batch['input_ids'].to(device)
        bbox=batch['bbox'].to(device)
        attention_mask=batch['attention_mask'].to(device)
        token_type_ids=batch['token_type_ids'].to(device)
        labels=batch['labels'].to(device)
        resized_images = batch['resized_image'].to(device) 
        resized_and_aligned_bounding_boxes = batch['resized_and_aligned_bounding_boxes'].to(device) 

        # forward pass
        outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids, 
                        labels=labels, resized_images=resized_images, resized_and_aligned_bounding_boxes=resized_and_aligned_bounding_boxes)

        # get the loss and logits
        tmp_eval_loss = outputs.loss
        logits = outputs.logits

        eval_loss += tmp_eval_loss.item()
        nb_eval_steps += 1

        # compute the predictions
        if preds is None:
            preds = logits.detach().cpu().numpy()
            out_label_ids = labels.detach().cpu().numpy()
        else:
            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)
            out_label_ids = np.append(
                out_label_ids, labels.detach().cpu().numpy(), axis=0
            )

# compute average evaluation loss
eval_loss = eval_loss / nb_eval_steps
preds = np.argmax(preds, axis=2)

out_label_list = [[] for _ in range(out_label_ids.shape[0])]
preds_list = [[] for _ in range(out_label_ids.shape[0])]

for i in range(out_label_ids.shape[0]):
    for j in range(out_label_ids.shape[1]):
        if out_label_ids[i, j] != -100:
            out_label_list[i].append(idx2label[out_label_ids[i][j]])
            preds_list[i].append(idx2label[preds[i][j]])

results = {
    "loss": eval_loss,
    "precision": precision_score(out_label_list, preds_list),
    "recall": recall_score(out_label_list, preds_list),
    "f1": f1_score(out_label_list, preds_list),
}
print(results)

"""The results I was getting are:
* {'loss': 0.7689214440492483, 'precision': 0.7918682331260078, 'recall': 0.8025916413728695, 'f1': 0.7971938775510204} (run 1)
* 'loss': 0.6488624169276311, 'precision': 0.8053668087066682, 'recall': 0.8163670324538874, 'f1': 0.8108296133109165} (run 2)

## Inference

Let's test out the model on one of the documents in the test set.
"""

from PIL import Image, ImageDraw, ImageFont

image = Image.open("/content/data/testing_data/images/83443897.png")
image = image.convert("RGB")
image

# let's create a dataset just for this image
inference_dataset = FUNSDDataset(image_file_names=["83443897.png"], tokenizer=tokenizer, max_length=512, target_size=224, train=False)
test_encoding = inference_dataset[0]
test_encoding.keys()

for k,v in test_encoding.items():
  test_encoding[k] = test_encoding[k].unsqueeze(0).to(device)

input_ids=test_encoding['input_ids']
bbox=test_encoding['bbox']
attention_mask=test_encoding['attention_mask']
token_type_ids=test_encoding['token_type_ids']
labels=test_encoding['labels']
resized_images = test_encoding['resized_image']
resized_and_aligned_bounding_boxes = test_encoding['resized_and_aligned_bounding_boxes']

for k,v in test_encoding.items():
  print(test_encoding[k].shape)

# forward pass to get logits
outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids, 
                labels=labels, resized_images=resized_images, resized_and_aligned_bounding_boxes=resized_and_aligned_bounding_boxes)

token_predictions = outputs.logits.argmax(-1).squeeze().tolist() # the predictions are at the token level
token_actual_boxes = test_encoding['unnormalized_token_boxes'].squeeze().tolist()

word_level_predictions = [] # let's turn them into word level predictions
final_boxes = []
for id, token_pred, box in zip(input_ids.squeeze().tolist(), token_predictions, token_actual_boxes):
  if (tokenizer.decode([id]).startswith("##")) or (id in [tokenizer.cls_token_id, 
                                                           tokenizer.sep_token_id, 
                                                          tokenizer.pad_token_id]):
    # skip prediction + bounding box

    continue
  else:
    word_level_predictions.append(token_pred)
    final_boxes.append(box)

print(word_level_predictions)
print(final_boxes)

# for id, label, pred in zip(input_ids.squeeze().tolist(), test_encoding['labels'].squeeze().tolist(), token_predictions):
#   print(tokenizer.decode([id]), label, pred)

"""Let's visualize the result!"""

draw = ImageDraw.Draw(image)

font = ImageFont.load_default()

def iob_to_label(label):
    return label[2:]

label2color = {'question':'blue', 'answer':'green', 'header':'orange', 'other':'violet'}

for prediction, box in zip(word_level_predictions, final_boxes):
    predicted_label = iob_to_label(idx2label[prediction]).lower()
    draw.rectangle(box, outline=label2color[predicted_label])
    draw.text((box[0] + 10, box[1] - 10), text=predicted_label, fill=label2color[predicted_label], font=font)

image

"""Compare to ground-truth:"""

image = Image.open("/content/data/testing_data/images/83443897.png")
image = image.convert('RGB')

draw = ImageDraw.Draw(image)

label2color = {'question':'blue', 'answer':'green', 'header':'orange', 'other':'violet'}

with open('/content/data/testing_data/annotations/83443897.json') as f:
  data = json.load(f)

for annotation in data['form']:
  label = annotation['label']
  general_box = annotation['box']
  draw.rectangle(general_box, outline=label2color[label], width=2)
  draw.text((general_box[0] + 10, general_box[1] - 10), label, fill=label2color[label], font=font)
  words = annotation['words']
  for word in words:
    box = word['box']
    draw.rectangle(box, outline=label2color[label], width=1)

image

